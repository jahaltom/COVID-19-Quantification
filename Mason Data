import yaml
import sys
import os
from pyrpipe import sra,quant
from pyrpipe.runnable import Runnable
import pandas as pd


####Read config#####
configfile: "config.yaml"
DIR = config['DIR']
THREADS=config['THREADS']
Tr=config['transcriptome']



#####Read in run_accession ids######
with open ("RAids.txt") as f:
        ra_ids=f.read().splitlines()

#create salmon object. If index is not present it will be created
salmon=quant.Salmon(index="human_data/salmon_index",transcriptome=Tr,threads=15)

rule all:
        input:
                expand("{wd}/results_TPM_tx.tsv",wd=DIR)
rule quant:
        output:
                quant_file="{wd}/{sample}/salmon_out/quant.sf"
        run:
                # Path to quant file
                outfile=str(output.quant_file)
                # get sample name {sample}
                srrid=outfile.split("/")[1]
                # Get path to bam file
                bam="/ocean/projects/mcb200036p/shared/02-AlignedData/" + srrid + "/"  + srrid + "_Aligned.sortedByCoord_sorted.out.bam"
                # Make sure bam file size is > 0
                b = os.path.getsize(bam)
                if b > 0:
                        #Pathway for fastq files and salmon output 
                        path=DIR + "/" + srrid+ "/"
                        #Fastq file names and path. 
                        fq1=path + srrid + '1.fastq'
                        fq2=path + srrid + '2.fastq'
                        #Run picard to convert bam to fastq
                        picard=Runnable(command='picard')
                        param={'SamToFastq':'','I=': bam ,'F=': fq1,'F2=': fq2}
                        picard.run(**param)
                        #Run Salmon on sra object(fastq files) and delete fastq when finished.  
                        sra.SRA(fastq=fq1,fastq2=fq2,directory=path).quant(salmon).delete_fastq()

rule merge:
	input:
		["{wd}/{sample}/salmon_out/quant.sf".format(wd=DIR,sample=s) for s in ra_ids]
	output:
		outfile="{wd}/results_TPM_tx.tsv"
	run:
		#read in file
		with open(input[0]) as f:
			thisdata=f.read().splitlines()
		thisdata.pop(0)		
        	#Run accession IDs
		names=[]
        	##Transcript IDs
		txids=[]       
		for l in thisdata:
            	#Get 1st column (TranscriptID)
			thistx=l.split('\t')[0]                    
			txids.append(thistx)
		##For TPM	
		dftpm=pd.DataFrame({'TranscriptID':txids})
        	##Make a copy for the counts
        	dfcount=pd.DataFrame({'TranscriptID':txids})
		#read files in memory
		for qf in input:
            		##current filename(RAid)
			name=qf.split('/')[1]
            		##Will become list of all RAids
			names.append(name)
            		##Get TPM
			thisdata=pd.read_csv(qf,sep='\t',usecols=[3],skiprows=0)
            		##Get Counts
            		counts=pd.read_csv(qf,sep='\t',usecols=[4],skiprows=0)
            		##Add TPM and counts to respective df with current Run accession ID as column name
			dftpm[name]=thisdata['TPM']
            		dfcount[name]=counts['NumReads']

        	#transcript TPMs. 
		df_tx=dftpm[['TranscriptID']+names].copy()
		#write to file
		df_tx.to_csv(output.outfile,sep='\t',index=False)
        
              
        	
		###Collapse transcript to respective gene names sum counts and TPMs
		
		#Gather counts and TPMs for each transcript
		df_gene_tpm=dftpm[['TranscriptID']+names].copy()	
        	df_gene_count=dfcount[['TranscriptID']+names].copy()
		
        
        	#Read in gene metadata
        	md=pd.read_csv('meta_data.tsv',sep='\t',skiprows=0) 
        	md.rename(columns={ md.columns[0]: "TranscriptID" }, inplace = True)
        	
		#Make df with transcript id and corresponding gene id
		md2=md[['TranscriptID','Gene_stable_ID']]
		##Merge TPM and count data by TranscriptID
		df_gene_tpm=md2.merge(df_gene_tpm, on=['TranscriptID'], how='right')       	      
        	df_gene_count=md2.merge(df_gene_count, on=['TranscriptID'], how='right')
		
		#Collapse so that each gene id is listed once. sum up corresponding transcript TPM and counts.
		df_gene_tpm = df_gene_tpm.groupby(['Gene_stable_ID'],as_index = False).sum()
        	df_gene_count = df_gene_count.groupby(['Gene_stable_ID'],as_index = False).sum()
		
		#Format metadata to have each gene ID once
        	del md['TranscriptID']
       		md=md.drop_duplicates()

     		
		##Merge metadata to counts and TPM
        	df_gene_tpm=md.merge(df_gene_tpm, on=['Gene_stable_ID'], how='right')        	    	
        	df_gene_count=md.merge(df_gene_count, on=['Gene_stable_ID'], how='right')
    
		#reorder so gene name is first.	
        	df_gene_tpm = df_gene_tpm[ ['Gene_name'] + [ col for col in df_gene_tpm.columns if col != 'Gene_name' ] ]
        	df_gene_count = df_gene_count[ ['Gene_name'] + [ col for col in df_gene_count.columns if col != 'Gene_name' ] ]
		
		#####Filter TPM
		#create a median column
		df_gene_tpm['median']=df_gene_tpm.median(axis=1)
		#Remove genes where TPM median < 1, except for SARs genes. 
		indexNames = df_gene_tpm[ (df_gene_tpm['median'] < 1) & (df_gene_tpm['chr'] != 'SARSCOV2_ASM985889v3') ].index
		df_gene_tpm.drop(indexNames , inplace=True)

		#calculate medians of median tpm dist for protein_coding, lncRNA, and EB genes
		med_med_pc=result.loc[df_gene_tpm['Gene_type'] == 'protein_coding']['median'].median()
		med_med_lnc=result.loc[df_gene_tpm['Gene_type'] == 'lncRNA']['median'].median()
		med_med_eb=result.loc[df_gene_tpm['Gene_type'] == 'EB_novel']['median'].median()
		#Remove EB genes where median TPM is less than that of the med_med_lnc
		indexNames = df_gene_tpm[(df_gene_tpm['Gene_type'] == 'EB_novel') & (df_gene_tpm['median'] < med_med_lnc) ].index
		df_gene_tpm.drop(indexNames , inplace=True)
		
		df_gene_tpm.to_csv(DIR+'/results_TPM_gene.tsv',sep='\t',index=False)        
        	df_gene_count.to_csv(DIR+'/results_Count_gene.tsv',sep='\t',index=False)
